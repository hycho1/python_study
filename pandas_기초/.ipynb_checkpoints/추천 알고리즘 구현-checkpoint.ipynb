{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e9e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96683042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['과일은', '과일이다', '바나나나', '뭐가']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 문서들\n",
    "documents = [\n",
    "    \"먹고 싶은 과일은 바나나\",\n",
    "    \"바나나는 먹기 좋은 과일이다\",\n",
    "    \"사과와 바나나 중에 뭐가 더 좋을까\",\n",
    "    \"바나나나 사과나 둘 다 좋은데, 사과가 더 좋아\"\n",
    "]\n",
    "\n",
    "# TF-IDF 변환기 생성\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF 행렬 계산\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# 단어 목록 출력\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# 키워드 추출\n",
    "keywords = []\n",
    "for doc in tfidf_matrix:\n",
    "    # 문서에서 가장 높은 TF-IDF 값을 가지는 단어의 인덱스 추출\n",
    "    max_tfidf_index = doc.argmax()\n",
    "    # 해당 인덱스에 해당하는 단어를 키워드 리스트에 추가\n",
    "    keywords.append(feature_names[max_tfidf_index])\n",
    "\n",
    "# 중복 제거를 위해 집합으로 변환 후 다시 리스트로 변환\n",
    "keywords = list(set(keywords))\n",
    "\n",
    "# 최종 키워드 출력\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "333fff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CDL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CDL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f1bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def extract_keywords(tokens, top_n=5):\n",
    "    fdist = FreqDist(tokens)\n",
    "    return fdist.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64621459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: [('language', 3), (',', 3), ('natural', 2), ('computers', 2), ('processing', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "keywords = extract_keywords(preprocessed_text)\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38774aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nltk>=3.8 (from textblob)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\cdl\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\cdl\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cdl\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cdl\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cdl\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   --------------------------------------- 626.3/626.3 kB 19.9 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 31.9 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26e54245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: [('language', 3), ('and', 3), ('natural', 2), ('of', 2), ('computers', 2)]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    # TextBlob 객체 생성\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # 각 단어의 빈도 계산\n",
    "    word_freq = {}\n",
    "    for word in blob.words:\n",
    "        if len(word) > 1:  # 한 글자인 단어는 제외\n",
    "            word = word.lower()  # 단어를 소문자로 변환\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    # 빈도 기준으로 정렬하여 상위 N개의 단어 추출\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return sorted_words\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "\n",
    "# 키워드 추출\n",
    "keywords = extract_keywords(text)\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "376fd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 구현\n",
    "\n",
    "travel_data = {\n",
    "    '여행지 1': ['자연 경관', '산책로', '휴양지'],\n",
    "    '여행지 2': ['역사적인 유적지', '문화 유산', '유럽 감성'],\n",
    "    '여행지 3': ['해변 리조트', '수영', '해수욕'],\n",
    "    '여행지 4': ['도심 번화가', '쇼핑', '음식'],\n",
    "}\n",
    "\n",
    "# 사용자의 관심사\n",
    "user_interests = '음식 먹기 좋은 곳'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5d39f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연 경관 산책로 휴양지', '역사적인 유적지 문화 유산 유럽 감성', '해변 리조트 수영 해수욕', '도심 번화가 쇼핑 음식']\n"
     ]
    }
   ],
   "source": [
    "# 각 여행지의 키워드를 하나의 문자열로 결합\n",
    "travel_descriptions = [' '.join(keywords) for keywords in travel_data.values()]\n",
    "print(travel_descriptions)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(travel_descriptions)\n",
    "\n",
    "# 입력된 관심사를 TF-IDF 벡터로 변환\n",
    "user_vector = tfidf_vectorizer.transform([user_interests])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(user_vector, tfidf_matrix)\n",
    "\n",
    "# 가장 유사한 여행지 추천\n",
    "similar_indices = cos_sim.argsort()[0][::-1]\n",
    "for idx in similar_indices:\n",
    "    if cos_sim[0][idx] > 0:  # 유사도가 0보다 큰 경우에만 출력\n",
    "        print(list(travel_data.keys())[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08162fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6225a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar keywords: ['자연 경관', '산책로', '휴양지']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 여행지 데이터 (여행지 이름과 키워드 리스트)\n",
    "travel_data = {\n",
    "    '여행지 1': ['자연 경관', '산책로', '휴양지'],\n",
    "    '여행지 2': ['역사적인 유적지', '문화 유산', '유럽 감성'],\n",
    "    '여행지 3': ['해변 리조트', '수영', '해수욕'],\n",
    "    '여행지 4': ['도심 번화가', '쇼핑', '음식'],\n",
    "}\n",
    "\n",
    "# 사용자가 입력한 값\n",
    "user_query = '자연 경관과 산책로가 있는 곳'\n",
    "\n",
    "# 각 여행지의 키워드를 하나의 문자열로 결합\n",
    "travel_descriptions = [' '.join(keywords) for keywords in travel_data.values()]\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(travel_descriptions)\n",
    "\n",
    "# 사용자 입력을 TF-IDF 벡터로 변환\n",
    "user_vector = tfidf_vectorizer.transform([user_query])\n",
    "\n",
    "# 각 여행지와 사용자 입력의 유사도 계산\n",
    "similarities = cosine_similarity(user_vector, tfidf_matrix)\n",
    "\n",
    "# 유사도가 가장 높은 키워드 찾기\n",
    "most_similar_index = similarities.argmax()\n",
    "\n",
    "# 가장 유사한 여행지의 키워드 가져오기\n",
    "most_similar_keywords = list(travel_data.values())[most_similar_index]\n",
    "\n",
    "# 사용자가 다시 검색해볼 수 있도록 키워드 출력\n",
    "print(\"Most similar keywords:\", most_similar_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(travel_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349575f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 사용자로부터 입력 받은 문장\n",
    "user_input = \"자연 경관과 산책로가 있는 곳\"\n",
    "\n",
    "# 단어 토큰화 및 명사 추출\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(user_input)\n",
    "pos_tags = pos_tag(tokens)\n",
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keywords = [word for word in nouns if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Extracted keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231568fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40170a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
