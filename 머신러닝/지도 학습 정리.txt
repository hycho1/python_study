[ 지도 학습 ]
- '훈련' 데이터와 '정답' 데이터를 주고 학습 Train
- 데이터는 Input => Target 입력 값과 목표 값으로 훈련을 시킴
- 회귀와 분류에 적용되는 방식

[ 분류 ]
- 범주형(클래스) 데이터를 사용
- 클래스 중 하나의 결과

[ 회귀 ]
- 회귀는 수치형 데이터를 사용
- 수치 결과



[ 데이터 전처리 ]
- 샘플링 과정에서 특정 요소나 집단이 과도 또는 과소 대표되는 현상, 모집단을 제대로 반영하지 못하게 되어 신뢰, 정확성 하락
- 모집단의 비율을 맞춰야 함 (무작위 표본 추출 / 층화 표본 추출 / 체계적 표본 추출(일정 간격))
- 스케일링: 비교 데이터 간 값의 범위(데이터 표현의 기준)가 다르면 결과를 도출하는데 가중치가 다른 것처럼 적용됨, 별도로 설정하는 가중치와 별개로 기본 데이터 간에는 동일한 Scale로 적용되어야 함



[ KNN ] : 타겟 데이터가 직선거리로 어떤 데이터와 몇 개(neighbor)가 가까운지 파악


[ 결정 계수 ] : 𝑅2=1−(타겟−예측)^2의 합/(타겟−평균)^2의 합
- 분류에서 score는 샘플을 정확하게 분류한 개수의 비율(정확도)
- 회귀에서 score는 결정계수(coefficent of determination)


[ 과적합 ]
- 과대적합 : 훈련 셋의 점수가 좋은데 테스트 셋의 점수가 나쁜 경우
- 과소적합 : 훈련 셋의 점수보다 테스트 셋의 점수가 더 높거나 / 두 점수 모두 낮은 경우
- 과소 적합의 경우 데이터 자체가 충분하지 않은 경우 발생할 수 있음


[ 선형회귀 ] : 비교적 간단하며 특성이 하나인 경우 일차방정식의 형태로 표현 가능
- 기울기와 절편값을 조정하여 최적의 직선을 구함


[ 다항회귀 ] : 단일 특성을 다항식을 통해 표현, 이차, 삼차방정식 등
- 예시
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))


[ 다중회귀 ] : 2개 이상의 특성들을 이용하는 회귀 분석


[ 특성 공학 ] : 기존의 특성을 사용하여 새로운 특성을 뽑아내는 것
- 사이킷런의 PolynomialFeatures 클래스 활용
- 예시
poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
결과 >> [[1. 2. 3. 4. 6. 9.]]


[ 규제(regularization) ] : 머신러닝 모델이 훈련 데이터셋을 너무 과도하게 학습하지 못하도록 제어하는 것
- 이는 모델이 과적합의 가능성, 모델 계산 자체의 부하, 특성이 많다는건 데이터간의 거리(상관, 관계 등을 말하는듯)가 멀어지기 때문에 
연관성을 찾기 힘듦, 그로 인해 모델의 예측이 해석 불가능할 가능성이 커짐

- 과대적합 해소, 각 특성에 곱해지는 계수의 크기를 감소시키는 것(가중치 하락)
- 특성 스케일링 : 계수의 크기 비율을 맞추는 것 / 특성1: 1~10, 특성2: 1~1000
- 사이킷런에서 제공하는 StandardScaler 클래스 사용(Normalization)
- 표준화(Standardization)와 정규화(Normalization)가 있음
- 정규화 : 회귀 분석에서 정규화라는건 학습에 적용한 특성들이 너무 많으면 위와 같은 문제가 발생하기 때문에 복잡도를 줄이는 것을 말함


(L1-Normaliztion, L2-Normaliztion)
학습할 때 학습 데이터에 따라서 weight(가중치)값이 커질수 있고 이는 과적합을 유발할수 있어, 정규화 과정이 필요


[ 릿지(L2) ] 
- L2 Norm 은 벡터 p, q 의 유클리디안 거리(직선 거리)(원 모양의 분포)
Cost function에 제곱한 가중치 값을 더해줌으로써 편미분 을 통해 back propagation 할 때 
Cost 뿐만 아니라 가중치 또한 줄어드는 방식으로 학습을 한다. 
특정 가중치가 비이상적으로 커지는 상황을 방지하고, Weight decay가 가능해진다. 
⇒ 전체적으로 가중치를 작아지게 하여 과적합을 방지. 


[ 라쏘(L1) ]
- L1 Norm 은 벡터 p, q 의 각 원소들의 차이의 절대값의 합 (마름모 모양의 분포)
더하기 전 좌측이 일반적인 cost function이고 여기에 가중치 절대값을 더해준다.
편미분 을 하면 w값은 상수값이 되어버리고, 그 부호에 따라 +-가 결정된다. 가중치가 너무 작은 경우는 상수 값에 의해서 weight가 0이 된다.
⇒  결과적으로 몇몇 중요한 가중치들만 남게 됨


* 회귀 분석에서 결과값은 각 feature의 가중치(기울기, 계수)와 예측치를 곱하고 절편까지 모두 더해주는 값이 됨


[ 로지스틱 회귀(분류 모델) ] : 선형 회귀와 동일한 선형 방정식이며 0 ~ 1(0%~100%) 사이 값을 가짐
- 이진 분류(시그모이드) : 0.5를 기준으로 0에 가까운 값, 1에 가까운 값 2가지로 분류 / 하나의 선형 방정식의 출력 값을 0~1로 압축
- 다중 분류(소프트맥스) : 여러 개의 선형 방정식의 출력 값을 0~1로 압축 / 전체 합이 1이 되도록 만듦


[ 점진적 학습(온라인 학습) ] : 기존 데이터로 훈련된 모델에 새로운 데이터만 추가 훈련이 가능함
- 확률적 경사 하강법 : 목표지점을 찾기위해 랜덤하게 경사를 따라 내려가는 방법
- 손실 함수(비용함수)로 만들어진 그래프에서 기울기가 최소인 지점을 찾는 것
- 즉, 미분 값이 0에 수렴 (= 양의 2차 방정식에서는 y절편에 수렴)
- 손실 함수 : 회귀에서 손실은 평균 오차의 합을 기반으로 판단, 분류에서 손실은 정답을 못 맞추는 것으로 판단

- 과정)
1) 훈련 세트에서 랜덤한 샘플을 고름
2) 경사를 조금 내려감
3) 1)을 반복 하여 전체 샘플을 모두 사용할 때까지 반복
4) 샘플을 모두 사용했다면 1에포크(epoch)
5) 경사를 다 내려오지 못했다면 1)부터 다시 진행
보통 1) ~ 4)를 수십~수백번 에포크를 진행

- 에포크와 과대/과소 적합
확률적 경사 하강법을 사용하는 모델은 에포크 회수에 따라 과소/과대적합 발생 가능
에포크 횟수가 적으면 덜 학습(경사 하강이 중도에 멈춤)
에포크 횟수가 많으면 완전 학습(훈련 세트에 잘 맞는 모델이 됨)


결정트리 분류/회귀

교차검증

그리드 서치


앙상블
 - 랜덤 포레스트
 - 엑스트라 트리
 - 그레디언트 부스팅
 - 히스토그램 기반 그레디언트 부스팅
 - XGBoost
 - LightGBM



 == 평가 지표 ==

