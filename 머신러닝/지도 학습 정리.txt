[ 지도 학습 ]
- '훈련' 데이터와 '정답' 데이터를 주고 학습 Train
- 데이터는 Input => Target 입력 값과 목표 값으로 훈련을 시킴
- 회귀와 분류에 적용되는 방식

[ 분류 ]
- 범주형(클래스) 데이터를 사용
- 클래스 중 하나의 결과

[ 회귀 ]
- 회귀는 수치형 데이터를 사용
- 수치 결과



[ 데이터 전처리 ]
- 샘플링 과정에서 특정 요소나 집단이 과도 또는 과소 대표되는 현상, 모집단을 제대로 반영하지 못하게 되어 신뢰, 정확성 하락
- 모집단의 비율을 맞춰야 함 (무작위 표본 추출 / 층화 표본 추출 / 체계적 표본 추출(일정 간격))
- 스케일링: 비교 데이터 간 값의 범위(데이터 표현의 기준)가 다르면 결과를 도출하는데 가중치가 다른 것처럼 적용됨, 별도로 설정하는 가중치와 별개로 기본 데이터 간에는 동일한 Scale로 적용되어야 함



[ KNN ] : 타겟 데이터가 직선거리로 어떤 데이터와 몇 개(neighbor)가 가까운지 파악


[ 결정 계수 ] : 𝑅2=1−(타겟−예측)^2의 합/(타겟−평균)^2의 합
- 분류에서 score는 샘플을 정확하게 분류한 개수의 비율(정확도)
- 회귀에서 score는 결정계수(coefficent of determination)


[ 과적합 ]
- 과대적합 : 훈련 셋의 점수가 좋은데 테스트 셋의 점수가 나쁜 경우
- 과소적합 : 훈련 셋의 점수보다 테스트 셋의 점수가 더 높거나 / 두 점수 모두 낮은 경우
- 과소 적합의 경우 데이터 자체가 충분하지 않은 경우 발생할 수 있음


[ 선형회귀 ] : 비교적 간단하며 특성이 하나인 경우 일차방정식의 형태로 표현 가능
- 기울기와 절편값을 조정하여 최적의 직선을 구함


[ 다항회귀 ] : 단일 특성을 다항식을 통해 표현, 이차, 삼차방정식 등
- 예시
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))


[ 다중회귀 ] : 2개 이상의 특성들을 이용하는 회귀 분석


[ 특성 공학 ] : 기존의 특성을 사용하여 새로운 특성을 뽑아내는 것
- 사이킷런의 PolynomialFeatures 클래스 활용
- 예시
poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
결과 >> [[1. 2. 3. 4. 6. 9.]]


[ 규제(regularization) ] : 머신러닝 모델이 훈련 데이터셋을 너무 과도하게 학습하지 못하도록 제어하는 것
- 이는 모델이 과적합의 가능성, 모델 계산 자체의 부하, 특성이 많다는건 데이터간의 거리(상관, 관계 등을 말하는듯)가 멀어지기 때문에 
연관성을 찾기 힘듦, 그로 인해 모델의 예측이 해석 불가능할 가능성이 커짐

- 과대적합 해소, 각 특성에 곱해지는 계수의 크기를 감소시키는 것(가중치 하락)
- 특성 스케일링 : 계수의 크기 비율을 맞추는 것 / 특성1: 1~10, 특성2: 1~1000
- 사이킷런에서 제공하는 StandardScaler 클래스 사용(Normalization)
- 표준화(Standardization)와 정규화(Normalization)가 있음
- 정규화 : 회귀 분석에서 정규화라는건 학습에 적용한 특성들이 너무 많으면 위와 같은 문제가 발생하기 때문에 복잡도를 줄이는 것을 말함


(L1-Normaliztion, L2-Normaliztion)
학습할 때 학습 데이터에 따라서 weight(가중치)값이 커질수 있고 이는 과적합을 유발할수 있어, 정규화 과정이 필요


[ 릿지(L2) ] 
- L2 Norm 은 벡터 p, q 의 유클리디안 거리(직선 거리)(원 모양의 분포)
Cost function에 제곱한 가중치 값을 더해줌으로써 편미분 을 통해 back propagation 할 때 
Cost 뿐만 아니라 가중치 또한 줄어드는 방식으로 학습을 한다. 
특정 가중치가 비이상적으로 커지는 상황을 방지하고, Weight decay가 가능해진다. 
⇒ 전체적으로 가중치를 작아지게 하여 과적합을 방지. 


[ 라쏘(L1) ]
- L1 Norm 은 벡터 p, q 의 각 원소들의 차이의 절대값의 합 (마름모 모양의 분포)
더하기 전 좌측이 일반적인 cost function이고 여기에 가중치 절대값을 더해준다.
편미분 을 하면 w값은 상수값이 되어버리고, 그 부호에 따라 +-가 결정된다. 가중치가 너무 작은 경우는 상수 값에 의해서 weight가 0이 된다.
⇒  결과적으로 몇몇 중요한 가중치들만 남게 됨


* 회귀 분석에서 결과값은 각 feature의 가중치(기울기, 계수)와 예측치를 곱하고 절편까지 모두 더해주는 값이 됨


[ 로지스틱 회귀(분류 모델) ] : 선형 회귀와 동일한 선형 방정식이며 0 ~ 1(0%~100%) 사이 값을 가짐
- 이진 분류(시그모이드) : 0.5를 기준으로 0에 가까운 값, 1에 가까운 값 2가지로 분류 / 하나의 선형 방정식의 출력 값을 0~1로 압축
- 다중 분류(소프트맥스) : 여러 개의 선형 방정식의 출력 값을 0~1로 압축 / 전체 합이 1이 되도록 만듦


[ 점진적 학습(온라인 학습) ] : 기존 데이터로 훈련된 모델에 새로운 데이터만 추가 훈련이 가능함
- 확률적 경사 하강법 : 목표지점을 찾기위해 랜덤하게 경사를 따라 내려가는 방법
- 손실 함수(비용함수)로 만들어진 그래프에서 기울기가 최소인 지점을 찾는 것
- 즉, 미분 값이 0에 수렴 (= 양의 2차 방정식에서는 y절편에 수렴)
- 손실 함수 : 회귀에서 손실은 평균 오차의 합을 기반으로 판단, 분류에서 손실은 정답을 못 맞추는 것으로 판단

- 과정)
1) 훈련 세트에서 랜덤한 샘플을 고름
2) 경사를 조금 내려감
3) 1)을 반복 하여 전체 샘플을 모두 사용할 때까지 반복
4) 샘플을 모두 사용했다면 1에포크(epoch)
5) 경사를 다 내려오지 못했다면 1)부터 다시 진행
보통 1) ~ 4)를 수십~수백번 에포크를 진행

- 에포크와 과대/과소 적합
확률적 경사 하강법을 사용하는 모델은 에포크 회수에 따라 과소/과대적합 발생 가능
에포크 횟수가 적으면 덜 학습(경사 하강이 중도에 멈춤)
에포크 횟수가 많으면 완전 학습(훈련 세트에 잘 맞는 모델이 됨)


[ 결정트리 분류/회귀 ] : 조건에 따른 분류 결정을 트리 구조로 표현
- 음성 클래스가 왼쪽, 양성 클래스가 오른쪽
- 리프 노드가 가장 많이 해당하는 클래스가 예측 대상(target)
- 트리의 깊이 => 분류를 반복하며 점차 세밀해짐
- 결정트리의 결과를 회귀에 적용하면 리프 노드에 도달한 샘플 타겟의 평균으로 예측

- 지니 불순도(Gini impurity) : 각 노드에서 하위 노드를 분류하기 위한 기준
- 결정트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 생성함
- 계층 간 불순도의 차이는 정보 이득(imformation gain)이라고 함
* 정보 이득이 커진다 : 내가 예측하려는 대상이 많으면 지니계수가 작아짐, 말그대로 잡다한 것이 섞여있지 않다는 뜻이니까
그렇다면 내가 처음에는 대략 나누고, 그 다음에는 최대한 자세하게 나누려고 해야하는 것,
기준이 더해질수록 더해진 조건을 충족하는 타겟을 찾게되니 불순도는 낮아진다.
즉, 불순도를 최대한 낮추는 방향을 잡겠다는 뜻


[ 교차검증 ] : 테스트 데이터를 분할해서 각 분할영역을 검증 데이터로 구성하여 체크하는 것
- 트레이닝 데이터 셋을 분할한 횟수에 따라 N-Fold라고 함


[ 하이퍼 파리미터 튜닝(Hyper Parameter Tuning) ]
- 머신러닝 모델이 학습하는 파라미터는 모델 파라미터
- 사용자가 지정해야 하는 파라미터는 하이퍼파라미터
- 모든 하이퍼파라미터는 클래스 또는 매개변수로 표현


[ 그리드 서치 ] : 지정한 하이퍼 파라미터의 개수별로 ex) 4가지 - N*M*i*j 식으로 반복 횟수를 늘려서 테스트를 진행 시켜줌
- 결정트리 모델에서 min_impurity_decrease 파라미터 리스트 준비
- 파라미터 리스트를 이용하여 GridSearchCV객체 생성
- GridSearchCV의 cv 디폴트 값은 5(총 5번 수행. 총 25번 진행)
- n_jobs는 병렬실행에 사용할 CPU코어 수 지정(기본값은 1. -1로 지정하면 모든 코어 사용)
- 각 수행한 학습 모델 간 교차 검증 가능
- 예제에서 사용한 기본 모델은 결정트리분류(DecisionTreeClassifier)


[ 랜덤 서치 ] : 그리드 서치가 지정한 범위의 파라미터였다면 랜덤 서치는 확률 분포를 사용하여 지정


[ *앙상블 ]

[ 랜덤 포레스트 ]
- 결정트리를 랜덤하게 생성해서 forest를 구성
- 각 결정트리의 예측 결과를 이용하여 최종 예측을 만듦

- 결정트리를 생성하기 위해 훈련 데이터를 랜덤하게 생성하는데
- 부트스트랩 샘플 수집 방식 : 데이터셋에서 중복을 허용하여 데이터 샘플링을하는 방식
- 기본적으로 추출된 샘플은 훈련 세트와 크기가 같음
- 각 노드 분할 시 전체 특성에서 일부 특성을 무작위로 골라서 최선의 분할을 찾음
- 기본적으로 100개의 결정트리를 위와 같은 방식으로 훈련 진행
- 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측에 사용
- 회귀일 때는 트리의 예측을 평균하여 사용

- 랜덤한 샘플과 특성을 이용하므로 과대적합 방지
- 검증 데이터 세트와 데트스 데이터 세트에서 안정적인 성능 기대 가능
- 디폴트 파라미터 값으로도 좋은 결과를 예상 가능