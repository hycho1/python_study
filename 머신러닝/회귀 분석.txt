데이터 분석에서의 회귀란 전체적인 데이터에 해당하는 표준편차를 기준으로 특정 타겟이나 (표준편차를 기준으로하는) 데이터들의 범위를 분석하는 것이라고 이해
'회귀한다'라는건 각 데이터 사이에서 크게 편차가 있더라도 종합적으로는 편차의 평균을 크게 벗어나지 않는 범위내에서 목표로 한 타겟이 존재할 것이다라고 가정하는 것


선형회귀는 데이터들의 표준편차를 최소화하는 방정식을 구하는게 목표인듯?
(y =ax + b)

단순 선형회귀으로 일차방정식을 얘기하는게 맞는지는 모르겠지만 다항회귀라고 2,3차 방정식의 형태로 구분되어 있음
이때 기울기(coefficient)나 절편(intercept)이 하이퍼파라미터가 되고 분석에 영향을 줌

feature를 늘린다는 건 결과값을 도출하는것에 영향을 주는 요소를 늘린다는 것이고
N차 방정식과 다르게 x이외의 feature를 추가하는 것 -> '다중회귀'라고 함
그리고 그렇게 feature가 늘어날때마다 차원이 늘었다고 표현함

(PolynomialFeatures)
기존 특성으로 새로운 특성을 뽑아내는걸 '특성 공학'이라고 하는데 feature끼리 곱한다던지, 제곱한다던지 등등으로 구성함
(데이터를 조작하는게 아닌가 하는 의구심이 듦, 그렇게 만들어서 score를 올리면 그것에 의미가 있을까?
> GPT 답변, 조작이 아니라 강화다)


데이터보다 특성의 개수가 더 많은 상황에서는 데이터를 규제 해야하는데 
이는 모델이 과적합의 가능성,
모델 계산 자체의 부하,
특성이 많다는건 데이터간의 거리(상관, 관계 등을 말하는듯)가 멀어지기 때문에 연관성을 찾기 힘듦,
그로 인해 모델의 예측이 해석 불가능할 가능성이 커짐
--> 특성 스케일링


릿지(L2)와 라쏘(L1)

회귀 분석에서 정규화라는건 학습에 적용한 특성들이 너무 많으면 위와 같은 문제가 발생하기 때문에 복잡도를 줄이는 것을 말한다

(L1-Normaliztion, L2-Normaliztion)
학습할 때 학습 데이터에 따라서 weight(가중치)값이 커질수 있고 이는 과적합을 유발할수 있어, 정규화 과정이 필요

- L1 Norm 은 벡터 p, q 의 각 원소들의 차이의 절대값의 합 (마름모 모양의 분포)
- L2 Norm 은 벡터 p, q 의 유클리디안 거리(직선 거리)(원 모양의 분포)

출처: https://esj205.oopy.io/4b321662-5d02-4559-8677-7e974cf080a8


L1
더하기 전 좌측이 일반적인 cost function이고 여기에 가중치 절대값을 더해준다.
편미분 을 하면 w값은 상수값이 되어버리고, 그 부호에 따라 +-가 결정된다. 가중치가 너무 작은 경우는 상수 값에 의해서 weight가 0이 된다.
⇒  결과적으로 몇몇 중요한 가중치들만 남게 됨
(미분으로 필터링하는건 생각치못했음)


L2
Cost function에 제곱한 가중치 값을 더해줌으로써 편미분 을 통해 back propagation 할 때 
Cost 뿐만 아니라 가중치 또한 줄어드는 방식으로 학습을 한다. 
특정 가중치가 비이상적으로 커지는 상황을 방지하고, Weight decay가 가능해진다. 
⇒ 전체적으로 가중치를 작아지게 하여 과적합을 방지. 


회귀 분석에서 결과값은 각 feature의 가중치(기울기, 계수)와 예측치를 곱하고 절편까지 모두 더해주는 값이 됨














